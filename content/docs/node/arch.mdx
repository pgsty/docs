---
title: Architecture
description: Node types, architecture, and core concepts
icon: Blocks
---

A ["**node**"](/docs/prepare/hardware#node) refers to a resource that is SSH [accessible](#accessibility) and offers a bare Linux OS environment.
It could be a physical machine, a virtual machine, or an OS-like container equipped with `systemd`, `sudo` and `sshd`.

There are three different types of nodes in Pigsty, In a [one-node deployment](/docs/install/start), they are the same one.

| Node Type                   | Description                                                      |
|-----------------------------|------------------------------------------------------------------|
| [Admin node](#admin-node)   | The node where Pigsty is installed and admin commands are issued |
| [Infra node](#infra-node)   | The node where the [`INFRA`](/docs/infra/) module is installed   |
| [Common node](#common-node) | Any nodes managed by Pigsty, including admin & infra nodes       |

------

## Example

In the following 4-node [sandbox](/docs/prepare/sandbox) config snippet, we have 4 common nodes.\
And the `10.10.10.10` is marked as Infra Node and Admin Node simultaneously.

```yaml
all:
  children:
    infra:   { hosts: { 10.10.10.10: { infra_seq: 1 } } }  # <--- mark this as infra node
    etcd:    { hosts: { 10.10.10.10: { etcd_seq: 1 } }, vars: { etcd_cluster: etcd } }
    pg-meta: { hosts: { 10.10.10.10: { pg_seq: 1, pg_role: primary } }, vars: { pg_cluster: pg-meta } }
    pg-test:
      hosts:
        10.10.10.11: { pg_seq: 1, pg_role: primary }
        10.10.10.12: { pg_seq: 2, pg_role: replica }
        10.10.10.13: { pg_seq: 3, pg_role: replica }
      vars: { pg_cluster: pg-test }
  vars:
    admin_ip: 10.10.10.10    # <--- mark this as admin node
```

------

## Common Node

Common nodes have the following components enabled by default in Pigsty:

|    Component    |  Port  | Description                      | Default |
|:---------------:|:------:|----------------------------------|---------|
| `node_exporter` | `9100` | Node Monitoring Metrics Exporter | Enabled |
|    `haproxy`    | `9101` | HAProxy admin / metrics port     | Enabled |
|   `promtail`    | `9080` | Log collecting agent             | Enabled |

These components are optional, can be enabled with parameters.

|       Component       |  Port  | Description                  | Default    |
|:---------------------:|:------:|------------------------------|------------|
|       `docker`        | `9323` | Enable Container Service     | *Disabled* |
|     `keepalived`      |  N/A   | Manage Node Cluster L2 VIP   | *Disabled* |
| `keepalived_exporter` | `9650` | Monitoring Keepalived Status | *Disabled* |



------

## ADMIN Node

[Admin](/docs/prepare/admin) Node is the first node where Pigsty is installed, all control commands are issued from it.

There is one and only one admin node in a pigsty deployment, which is specified by [`admin_ip`](/docs/infra/param/#admin_ip).
This parameter will be set during the [configure](/docs/config/configure) procedure, to the value of primary IP address.

<Callout title="Secure admin node access" type="warn">

    The admin node should have nopass [`ssh`](/docs/prepare/admin#ssh) and [`sudo`](/docs/prepare/admin#sudo-privileges) access to all other nodes in the environment,
    which is dangerous when exposed to unauthorized access. Please keep it safe.

</Callout>

<Callout title="Admin node is the first node where Pigsty is installed" type="info">

    The admin node is the first node where Pigsty is installed, and it is used to issue commands to other nodes.\
    The admin node is usually overlapped with the infra node, same as the first infra node

</Callout>

<Callout title="It is possible to use your local laptop as admin node" type="info">

    It is possible to install pigsty on your local laptop / macbook, install ansible and issue commands from there.

</Callout>

------

## INFRA Node

A pigsty deployment may have one or more infra nodes, at least 1, 2 or more for production deployment.

The `infra` group specifies infra nodes in the inventory.
And infra nodes will have [INFRA](/docs/infra/) module installed (DNS, Nginx, Prometheus, Grafana, etcâ€¦).

|      Component      |  Port  |   Domain   | Description                       |
|:-------------------:|:------:|:----------:|-----------------------------------|
|       `nginx`       |  `80`  | `h.pigsty` | Web Service Portal (YUM/APT Repo) |
|   `alertmanager`    | `9093` | `a.pigsty` | Alert Aggregation and delivery    |
|    `prometheus`     | `9090` | `p.pigsty` | Monitoring Time Series Database   |
|      `grafana`      | `3000` | `g.pigsty` | Visualization Platform            |
|        `lok`        | `3100` |     -      | Logging Collection Server         |
|    `pushgateway`    | `9091` |     -      | Collect One-Time Job Metrics      |
| `blackbox_exporter` | `9115` |     -      | Blackbox Probing                  |
|      `dnsmasq`      |  `53`  |     -      | DNS Server                        |
|      `chronyd`      | `123`  |     -      | NTP Time Server                   |
|      `ansible`      |   -    |     -      | Run playbooks                     |



------

## PGSQL Node

The node with [PGSQL](/docs/pgsql/) module installed is called a PGSQL node. The node and PG instance are 1:1 deployed.
And node identities can be borrowed from pg instances with [`node_id_from_pg`](/docs/node/param/#node_id_from_pg) option.

Here are additional components & service port on PGSQL nodes:

|       Component       |  Port  | Description                                  | Status   |
|:---------------------:|:------:|----------------------------------------------|----------|
|      `postgres`       | `5432` | PostgreSQL Server Process Managed by Patroni | Enabled  |
|      `pgbouncer`      | `6432` | Pgbouncer Connection Pool                    | Enabled  |
|       `patroni`       | `8008` | Patroni HA Component, Manage `postgres`      | Enabled  |
| `primary` @ `haproxy` | `5433` | Primary connection pool: Read/Write Service  | Enabled  |
| `replica` @ `haproxy` | `5434` | Replica connection pool: Read-only Service   | Enabled  |
| `default` @ `haproxy` | `5436` | Primary Direct Connect Service               | Enabled  |
| `offline` @ `haproxy` | `5438` | Offline Direct Connect: Offline Read Service | Enabled  |
|     `pg_exporter`     | `9630` | postgres Monitoring Metrics Exporter         | Enabled  |
| `pgbouncer_exporter`  | `9631` | pgbouncer Monitoring Metrics Exporter        | Enabled  |
| `pgbackrest_exporter` | `9854` | pgbackrest Monitoring Metrics Exporter       | Enabled  |
|     `vip-manager`     |   -    | Bind VIP to the primary                      | Disabled |

